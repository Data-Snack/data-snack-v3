# Data Snack v2: Complete Architecture & Migration Plan

## ğŸ—ï¸ New Architecture Overview

```
data-snack-v2/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ web/                    # Main Next.js 15 app
â”‚   â”œâ”€â”€ docs/                   # Documentation site
â”‚   â””â”€â”€ admin/                  # Analytics dashboard
â”‚
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ @data-snack/core/      # Core business logic
â”‚   â”œâ”€â”€ @data-snack/ui/        # Design system components
â”‚   â”œâ”€â”€ @data-snack/tracking/  # Tracking SDK
â”‚   â”œâ”€â”€ @data-snack/snacks/    # Snack components
â”‚   â”œâ”€â”€ @data-snack/database/  # Database schemas & types
â”‚   â””â”€â”€ @data-snack/config/    # Shared configurations
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ tracking-server/       # GTM Server Container
â”‚   â”œâ”€â”€ analytics-api/         # Analytics processing
â”‚   â””â”€â”€ stream-processor/      # Kafka/Flink setup
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ terraform/             # Infrastructure as Code
â”‚   â”œâ”€â”€ docker/                # Container definitions
â”‚   â””â”€â”€ k8s/                   # Kubernetes configs
â”‚
â””â”€â”€ tools/
    â”œâ”€â”€ scripts/               # Build & deploy scripts
    â””â”€â”€ generators/            # Code generators
```

## ğŸ“¦ Package Structure

### @data-snack/core
```typescript
// packages/core/src/index.ts
export * from './analytics'
export * from './consent'
export * from './personality'
export * from './gamification'

// Analytics Engine
export interface AnalyticsEngine {
  track(event: TrackingEvent): Promise<void>
  identify(userId: string, traits?: UserTraits): Promise<void>
  page(name: string, properties?: PageProperties): Promise<void>
}

// Consent Manager
export interface ConsentManager {
  getConsent(): ConsentState
  updateConsent(categories: ConsentCategories): void
  isConsentGiven(category: ConsentCategory): boolean
}

// Personality Engine
export interface PersonalityEngine {
  analyze(data: BehavioralData): PersonalityProfile
  getArchetype(profile: PersonalityProfile): PersonalityArchetype
  calculateMarketValue(profile: PersonalityProfile): MarketValue
}
```

### @data-snack/ui - Design System
```typescript
// packages/ui/src/components/index.ts
export { SnackContainer } from './SnackContainer'
export { MetaCommentary } from './MetaCommentary'
export { LiveDataStream } from './LiveDataStream'
export { PersonalityCard } from './PersonalityCard'
export { AchievementBadge } from './AchievementBadge'
export { ShareCard } from './ShareCard'
export { ConsentBanner } from './ConsentBanner'
export { DataVisualizer } from './DataVisualizer'



### @data-snack/tracking - Universal Tracking SDK
```typescript
// packages/tracking/src/index.ts
export class DataSnackSDK {
  private queue: TrackingEvent[] = []
  private config: SDKConfig
  private consent: ConsentState
  
  constructor(config: SDKConfig) {
    this.config = config
    this.initialize()
  }
  
  // Core tracking methods
  track(event: string, properties?: Record<string, any>): void {
    if (!this.consent.analytics) return
    
    const trackingEvent: TrackingEvent = {
      event,
      properties,
      timestamp: Date.now(),
      sessionId: this.getSessionId(),
      userId: this.getUserId(),
      context: this.getContext()
    }
    
    this.queue.push(trackingEvent)
    this.flush()
  }
  
  // Advanced tracking
  trackClickDNA(pattern: ClickPattern): void
  trackScrollBehavior(behavior: ScrollBehavior): void
  trackEngagement(metrics: EngagementMetrics): void
  trackPrivacyLeaks(leaks: PrivacyLeak[]): void
  
  // Server-side sync
  private async flush(): Promise<void> {
    if (this.queue.length === 0) return
    
    const events = [...this.queue]
    this.queue = []
    
    await fetch(`${this.config.endpoint}/track`, {
      method: 'POST',
      body: JSON.stringify({ events }),
      keepalive: true
    })
  }
}
```
Empfohlene Architektur (Dual-Rail)

Rail A â€“ Privacy-Baseline (lÃ¤uft immer, ohne Cookies)

Serverseitige Logs/Edge-Telemetry (Vercel Web Analytics o. vereinfachtes Server-Event via API Route)

Optional: Plausible (ohne Cookies, anonymisiert) â€“ nur Grundmetriken, Zero-Maintenance

Rail B â€“ Voll-Stack Analytics (nach Consent, maximal lehrreich)

CMP (Consent Management, TCF v2-fÃ¤hig)
z. B. Cookiebot/Sourcepoint/Consentmanager (TCF) oder OSS Klaro! (wenn TCF-Vendor-Liste nicht nÃ¶tig).
â†’ Steuert jede Tech per Consent Mode v2 & Data Layer.

Tag Orchestration
GTM Web + GTM Server-Side (sGTM auf Cloud Run)
â€“ First-Party Cookies, Server-seitige Enrichment, Weiterleitung an Tools.

Marketing/Web-Analytics
GA4 (Consent Mode v2, Measurement Protocol â†’ sGTM), fÃ¼r Kampagnen, Attribution, Benchmarks.

Product/Behavior Analytics & Replay
PostHog (Cloud oder Self-host; Session Replay, Funnels, Feature Flags)
â€“ Alternative/ErgÃ¤nzung: OpenReplay (reiner Replay, OSS).

Event-Pipeline/CDP (frei & flexibel)
RudderStack OSS oder Snowplow OSS â†’ verteilt Events an GA4, PostHog, Warehouse.

Warehouse / Lake
BigQuery (Free Tier) oder DuckDB + Parquet (kostenlos, im Repo/Storage)
â€“ fÃ¼r â€Single Source of Truthâ€œ, Re-Analysen, Ã¶ffentliche Notebooks.

BI/Reporting
Looker Studio (GA4/BQ), Metabase (OSS) fÃ¼r interne Dashboards.

Governance & QualitÃ¤t
Versionierte Event-Schemas (JSON Schema) im Repo, Linting im CI, Event-Taxonomie.

Ergebnis: Du kannst live zeigen, wie Consent die Daten Ã¤ndert, wie sGTM Cookies first-party setzt, wie Events durch RudderStack/Snowplow ins Warehouse laufen â€“ und wie sich GA4 vs. PostHog anfÃ¼hlen. Genau die Lern-/Demowirkung, die du willst.

Was wird getrackt? (saubere Event-Taxonomie)

Core (alle Snacks): page_view, outbound_click, share_click, recipe_step_view, chart_interact (hover/brush/toggle), download_asset, cta_subscribe, search, filter_change.
Meta-Kontext: snack_id, topic, data_version, viz_id, ab_test_variant.
IDs: device_id (sGTM 1st-party), optional user_id (Login), pseudonymisiert & salted (hash(email+salt)).

IdentitÃ¤t & Consent (rechtssicher + lehrreich)

Consent Mode v2: GA4 pings ohne Cookies bis Zustimmung; nach Zustimmung volle Cookies/Attribution.

Server-seitige Cookies Ã¼ber sGTM (httponly, first-party).

Regionale Controls (EEA-Enforcement, IP-Anonymisierung).

Telemetry Panel (eigene Seite): zeigt live, welche Tags/Endpoints gerade feuern â€“ super fÃ¼rs Publikum.

Warum Plausible trotzdem drin bleibt

Resilienz & Vergleichbarkeit: LÃ¤uft ohne Consent, ohne Cookies â†’ du hast immer Grundmetriken (Traffic-Trend, Top-Pages).

Didaktik: Zeig den Unterschied â€Privacy-Baselineâ€œ vs. â€Voll-Stack mit Consentâ€œ.

Zero-Ops: Nix zu warten; parallel zu allem anderen.

Konkreter Stack (zusammengefasst)

CMP: TCF v2-fÃ¤hig (Cookiebot/Sourcepoint/Consentmanager) oder OSS Klaro!

Orchestration: GTM Web + GTM Server-Side (Cloud Run)

Web/Marketing: GA4 (Consent Mode v2, MP â†’ sGTM)

Product & Replay: PostHog (Events + Replay) (alternativ: OpenReplay nur fÃ¼r Replay)

CDP/Pipeline: RudderStack OSS oder Snowplow OSS

Warehouse: BigQuery (oder DuckDB+Parquet fÃ¼r kostenfrei/offen)

Dashboards: Looker Studio, Metabase

Baseline: Plausible (optional, aber empfehlenswert)

Rollout-Plan (schnell & sauber)

Event-Taxonomie & Schemas im Repo festziehen (JSON Schema + Doku).

CMP einbauen â†’ Data Layer/Consent Mode v2 verdrahten.

GTM Web + sGTM aufsetzen; GA4 via Measurement Protocol â†’ sGTM.

RudderStack/Snowplow zwischen sGTM und Zielen (GA4, PostHog, Warehouse) schalten.

Warehouse/BI anbinden (BQ/Parquet â†’ Looker/Metabase).

Telemetry Panel & Consent-Demo Seite bauen.

QA (Tag-Debugger, Network, sGTM logs), CI-Checks fÃ¼r Schemas.

Kosten-/Ops-Hinweise

Kostenfrei/Low-Cost geht: RudderStack OSS, DuckDB/Parquet, Metabase OSS, Klaro! OSS.

Cloud-Komfort (GA4, BigQuery, PostHog Cloud) spart Ops-Zeit und ist fÃ¼r ein Ã¶ffentliches Lern-/Showcase-Projekt oft sinnvoll.

sGTM verursacht geringe Cloud-Run-Kosten â€“ aber der Lerneffekt (First-Party, Enrichment, Kontrolle) ist Gold wert.

Fazit

Plausible war nie als â€Ersatzâ€œ gedacht, sondern als robuste Grundspur. FÃ¼r dein Ziel (â€alles, was heute gehtâ€œ) empfehle ich klar das Dual-Rail-Setup oben. Damit sammelst du wertvolle Daten, bleibst rechtssicher, und kannst den gesamten modernen Analytics-Stack sichtbar demonstrieren â€“ genau das Signature-Piece fÃ¼r Data Snack.

Deine Architektur ist schon extrem durchdacht:

Frontend: Next.js + TypeScript + Tailwind + D3/Three.js â†’ perfekt fÃ¼r interaktive, visuell starke Snacks.

Backend: Node.js + Fastify + PostgreSQL/Timescale + Kafka/Flink â†’ leistungsstark, skalierbar, passt zur Event-Driven-Architektur.

Infra: Docker + Kubernetes + Terraform + CI/CD mit GitHub Actions â†’ professionell, zukunftssicher.

Analytics & Tracking: GTM Server-Side + Consent Mode v2 + Segment CDP + Custom SDK â†’ genau das, was du willst: Privacy und volle Tracking-Power.

Meine EinschÃ¤tzung zum Stack

Der Stack ist absolut okay â€“ er deckt sowohl klassische Webentwicklung als auch Big-Data-Tracking ab. Er ist sogar eher ambitioniert fÃ¼r ein Einzelprojekt, aber genau deshalb eignet er sich als â€Showcaseâ€œ, was heute mÃ¶glich ist.

Verbesserungspotenzial (eher Feinjustierung als â€deutlich besserâ€œ):

Segment CDP: super stark, aber teuer und proprietÃ¤r. Wenn du maximale Transparenz + OSS willst, wÃ¤re RudderStack OSS oder Snowplow die klarere Botschaft.

Flink: heavy fÃ¼r den Start. Du kÃ¶nntest erst mit Kafka + ksqlDB oder sogar simplen Postgres-Views loslegen, dann Flink als â€Upgradeâ€œ zeigen.

PostHog (statt oder zusÃ¤tzlich zu Segment): Open Source, bietet Session Replay, Funnels, A/B-Testing. KÃ¶nnte fÃ¼r â€Psychological Profilingâ€œ-Snacks spannender sein.

Infra: Kubernetes ist nice, aber fÃ¼r MVP reicht GCP Cloud Run (wie du ja schon andeutest) â€“ K8s kann spÃ¤ter kommen, wennâ€™s wirklich Multi-Service-Last braucht.

Warum das passt:
Dein Projekt will zeigen, was Big Tech macht. Deshalb ist es fast Pflicht, den Stack so â€Big Tech-mÃ¤ÃŸigâ€œ aufzuziehen â€“ Kafka, Flink, sGTM, Consent Mode. Du kannst dann bewusst demonstrieren: â€So lÃ¤uft es in real life â€“ aber hier seht ihr es transparent.â€œ

ğŸ‘‰ Fazit: Dein Stack ist genau richtig, weil er didaktisch, realistisch und erweiterbar ist. FÃ¼r die erste Version kannst du kleinere Bausteine wie Segment â†’ RudderStack/PostHog und Flink â†’ ksqlDB vereinfachen. Aber strategisch ist es perfekt auf deine Projektvision abgestimmt.

Willst du, dass ich dir einen konkreten Vorschlag fÃ¼r die Minimal-Version (MVP) des Stacks mache â€“ also welche Teile du fÃ¼r den ersten Launch wirklich brauchst und welche du spÃ¤ter andocken kannst?

You said:
ok
ChatGPT said:
Thought for 32s

Perfekt â€” dann liefere ich dir direkt den konkreten MVP-Stack + Umsetzungsplan, der dein â€Voll-Trackingâ€œ zeigt, aber in einer Woche sauber live gehen kann. Danach kannst du StÃ¼ck fÃ¼r StÃ¼ck â€aufdrehenâ€œ (Kafka/Flink/CDP), ohne etwas wegzuwerfen.

MVP-Stack (Launch-fÃ¤hig in 1 Woche)

Frontend

Next.js (App Router) + TypeScript + Tailwind + shadcn/ui

Client-SDK (leichtgewichtig): @data-snack/tracking (eigene Mini-Lib: queue, context, flush)

Consent & Orchestration

CMP: Klaro! (OSS) oder Cookiebot (wenn TCF-Vendor-Liste gewÃ¼nscht)

Consent Mode v2 Signale â†’ Data Layer

GTM Web + GTM Server-Side (sGTM) auf Cloud Run (First-party Cookies, Enrichment, Routing)

Analytics

GA4 (Consent Mode v2, Measurement Protocol â†’ sGTM)

PostHog (Events + Funnels + Session Replay) â€“ OSS/Cloud, extrem lehrreich

Baseline ohne Consent: schlankes serverseitiges /api/telemetry fÃ¼r Pageviews (nur TTFB/UA/IP-gekÃ¼rzt) â†’ garantiert Grundmetriken, selbst wenn Consent abgelehnt

Storage / BI

BigQuery (kostenarm, direkt mit GA4/MP oder sGTM Connector)

Optional parallel: DuckDB + Parquet (fÃ¼r Open-Notebooks & Reproduzierbarkeit)

Was du jetzt nicht brauchst (aber spÃ¤ter andocken kannst)

Kafka/Flink/CDP (RudderStack/Snowplow): als â€Phase 2â€œ einschalten, sobald MVP stabil ist

Kubernetes: Cloud Run reicht; K8s folgt, wenn mehr Services nÃ¶tig sind

Events & Taxonomie (MVP-Scope)

Core Events (fÃ¼r alle Snacks)

page_view, snack_start, snack_complete, chart_interact, cta_subscribe, outbound_click, share_click

Kontextfelder

snack_id, snack_version, viz_id, data_version, ab_variant, consent_state, device_id (1st-party), session_id

Beispiel (JSON, client â†’ sGTM)

{
  "event": "chart_interact",
  "timestamp": 1735741200123,
  "user": { "device_id": "dsn_abc123", "session_id": "sess_789" },
  "context": {
    "snack_id": "privacy-leak-detector",
    "snack_version": "2.0.0",
    "viz_id": "fp-uniqueness-donut",
    "consent_state": { "analytics": true, "marketing": false }
  },
  "properties": { "interaction": "toggle_series", "series": "webgl_gpu" }
}

Minimaler Technik-Pfad (Step-by-Step)
1) Consent & Data Layer (Tag-ready)

CMP einbauen (Klaro!/Cookiebot)

Data Layer: window.dataLayer.push({event:'consent', analytics_storage:'granted|denied', ...});

Consent Mode v2 init (default denied; EEA erzwingen)

2) GTM Web + sGTM (Cloud Run)

Web-Container: Tags fÃ¼r

GA4 (Config + Events) â€“ an Consent gebunden

PostHog (via sGTM HTTP Proxy oder direkt Client, aber nur nach Consent)

Fallback /api/telemetry (lÃ¤uft immer, ohne Cookies, nur Basisdaten)

sGTM-Container:

GA4 Measurement Protocol (Request-Transformation, IP-KÃ¼rzung, UA, Geo-approx)

PostHog Ingest (Server-Side Endpoint)

Optionale Persistenz (BigQuery Export Task/Cloud Function â€“ kann Phase 2 sein)

3) Client-SDK (10 Zeilen, aber sauber)

Queue + Context + navigator.sendBeacon/keepalive POST

track(event, properties) feuert:

Wenn consent.analytics === true â†’ fetch('/gtm/collect' or sGTM endpoint)

Sonst nur /api/telemetry (server-seitig anonymisiert)

4) API Telemetry (Baseline ohne Cookies)

Next.js Route Handler /api/telemetry:

nimmt page/referrer/ttfb an, speichert minimal in BigQuery (oder loggt zunÃ¤chst)

IP auf /24 (IPv4) oder /56 (IPv6) kappen, User-Agent hashen (salted), keine Cookies

5) BigQuery & Dashboards

GA4 â†’ BigQuery aktivieren (Export)

Looker Studio: 1 MVP-Dashboard

Reiter A: â€Ohne Consentâ€œ vs. â€Mit Consentâ€œ (Vergleichs-KPIs)

Reiter B: Snack-Funnel (Start â†’ Interact â†’ Complete â†’ Share)

Telemetry-Panel (Publikums-Feature)

Baue eine Seite /telemetry:

Laufende Liste der gefeuerten Tags (Web + sGTM), Consent-Status, Ziele (GA4/PostHog/Telemetry)

Network-Requests (Name, Payload-Preview, Status) â†’ Didaktischer Hit

Toggle: â€Simuliere Consent an/ausâ€œ (clientseitig) â†’ zeig Live-Unterschied

Security & Privacy Defaults (MVP)

First-party Cookies via sGTM; HttpOnly + SameSite=Lax, knappe TTL

IP-Kappung + UA-Hash serverseitig (egal ob Consent)

CSP, Referrer-Policy, Permissions-Policy (Mic/Camera/Geo off by default)

Delete-Me Endpoint: /api/gdpr/delete (device/session-basiert fÃ¼r MVP)

Data Retention: 90 Tage personenbezogen; anonym unbefristet

Env & Checkliste

Env

GTM_WEB_CONTAINER_ID

GTM_SERVER_URL

GA4_MEASUREMENT_ID, GA4_API_SECRET

POSTHOG_PROJECT_KEY, POSTHOG_HOST

BQ_PROJECT/SET/DATASET (optional in Phase 1)

Go-Live-Checks

Consent toggeln â†’ Requests vergleichen (Netzwerk-Tab)

GA4 DebugView â†’ Events & Params landen?

PostHog â†’ Session Replay nur bei Consent?

/api/telemetry â†’ wird immer gefÃ¼llt (anonym)?

Phasen danach (auf Knopfdruck)

Phase 2 (Advanced)

CDP: RudderStack OSS oder Snowplow (Server-Side in sGTM einspeisen)

Streams: Kafka + ksqlDB â†’ spÃ¤ter Flink

Warehouse: Roh-Events (JSON/Parquet) + dbt-Modelle

A/B-Tests: PostHog Feature Flags + GA4 Experiments

Phase 3 (Perf/Scale)

K8s, Service Mesh, Multi-Region; Re-Id Pipeline fÃ¼r eingeloggte User; ML-Features

Kurzfazit

Mit dem MVP oben zeigst du sofort: Consent-Impact, sGTM-Power, GA4 vs. PostHog, Basistelemetrie ohne Consent.

Und du hast eine saubere Rampe, um spÃ¤ter Kafka/Flink/CDP anzuschalten â€“ ohne Umbau.

###  Foundation 
```bash
# 1. Setup Monorepo
pnpm init
pnpm add -D turbo
pnpm add -D @changesets/cli

# 2. Create packages
mkdir -p packages/{core,ui,tracking,snacks,database}
mkdir -p apps/{web,docs,admin}
mkdir -p services/{tracking-server,analytics-api}

# 3. Setup shared TypeScript config
cat > packages/config/tsconfig/base.json << EOF
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "ESNext",
    "moduleResolution": "bundler",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "jsx": "react-jsx"
  }
}
EOF

# 4. Setup Turborepo
cat > turbo.json << EOF
{
  "pipeline": {
    "build": {
      "dependsOn": ["^build"],
      "outputs": ["dist/**", ".next/**"]
    },
    "dev": {
      "persistent": true,
      "cache": false
    },
    "test": {
      "dependsOn": ["build"],
      "cache": false
    }
  }
}
EOF
```

### Core Infrastructure 

#### Server-Side Tracking Setup
```typescript
// services/tracking-server/src/index.ts
import { GTMServer } from '@google/tag-manager-server'
import { ConsentMode } from '@google/consent-mode-v2'
import { Kafka } from 'kafkajs'

export class TrackingServer {
  private gtm: GTMServer
  private kafka: Kafka
  private consent: ConsentMode
  
  async initialize() {
    // Setup GTM Server Container
    this.gtm = new GTMServer({
      containerId: process.env.GTM_CONTAINER_ID,
      serverUrl: process.env.GTM_SERVER_URL
    })
    
    // Setup Kafka producer
    this.kafka = new Kafka({
      clientId: 'tracking-server',
      brokers: [process.env.KAFKA_BROKER]
    })
    
    // Setup Consent Mode v2
    this.consent = new ConsentMode({
      default: {
        analytics_storage: 'denied',
        ad_storage: 'denied',
        ad_user_data: 'denied',
        ad_personalization: 'denied'
      },
      region: ['EU', 'UK']
    })
  }
  
  async handleRequest(req: Request): Promise<Response> {
    const event = await this.parseEvent(req)
    
    // Check consent
    const consentState = await this.consent.check(event.userId)
    
    // Process based on consent
    if (consentState.analytics_storage === 'granted') {
      // Full tracking
      await this.gtm.send(event)
      await this.kafka.send({
        topic: 'events.raw',
        messages: [{ value: JSON.stringify(event) }]
      })
    } else {
      // Anonymous tracking only
      const anonymized = this.anonymize(event)
      await this.kafka.send({
        topic: 'events.anonymous',
        messages: [{ value: JSON.stringify(anonymized) }]
      })
    }
    
    return new Response('OK', { status: 204 })
  }
}
```

#### Database Schema (Supabase)
```sql
-- Core tables
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_stat_statements";

-- Users table (GDPR compliant)
CREATE TABLE users (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  fingerprint_hash TEXT UNIQUE,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  deletion_requested_at TIMESTAMPTZ,
  
  -- Consent management
  consent_version INTEGER DEFAULT 1,
  consent_categories JSONB DEFAULT '{
    "necessary": true,
    "analytics": false,
    "marketing": false,
    "personalization": false
  }'::jsonb,
  consent_updated_at TIMESTAMPTZ DEFAULT NOW(),
  
  -- Profile
  personality_type TEXT,
  total_xp INTEGER DEFAULT 0,
  achievements JSONB DEFAULT '[]'::jsonb,
  
  -- Privacy
  data_export_requested_at TIMESTAMPTZ,
  last_export_at TIMESTAMPTZ
);

-- Events table (partitioned by day)
CREATE TABLE events (
  id BIGSERIAL,
  user_id UUID REFERENCES users(id) ON DELETE CASCADE,
  session_id UUID NOT NULL,
  event_type TEXT NOT NULL,
  event_name TEXT NOT NULL,
  properties JSONB,
  context JSONB,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  
  PRIMARY KEY (id, created_at)
) PARTITION BY RANGE (created_at);

-- Create partitions for next 30 days
DO $$
BEGIN
  FOR i IN 0..30 LOOP
    EXECUTE format('
      CREATE TABLE IF NOT EXISTS events_%s PARTITION OF events
      FOR VALUES FROM (%L) TO (%L)',
      to_char(CURRENT_DATE + i, 'YYYY_MM_DD'),
      CURRENT_DATE + i,
      CURRENT_DATE + i + 1
    );
  END LOOP;
END $$;

-- Snack sessions
CREATE TABLE snack_sessions (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id UUID REFERENCES users(id) ON DELETE CASCADE,
  snack_id TEXT NOT NULL,
  started_at TIMESTAMPTZ DEFAULT NOW(),
  completed_at TIMESTAMPTZ,
  data_collected JSONB,
  results JSONB,
  personality_analysis JSONB,
  market_value DECIMAL(10,2),
  shared BOOLEAN DEFAULT FALSE,
  
  INDEX idx_snack_sessions_user (user_id),
  INDEX idx_snack_sessions_snack (snack_id)
);

-- Real-time views
CREATE MATERIALIZED VIEW hourly_stats AS
SELECT 
  date_trunc('hour', created_at) as hour,
  COUNT(DISTINCT user_id) as unique_users,
  COUNT(*) as total_events,
  COUNT(DISTINCT session_id) as sessions,
  AVG((properties->>'duration')::int) as avg_duration
FROM events
WHERE created_at > NOW() - INTERVAL '24 hours'
GROUP BY 1;

-- Refresh every 5 minutes
CREATE OR REPLACE FUNCTION refresh_hourly_stats()
RETURNS void AS $$
BEGIN
  REFRESH MATERIALIZED VIEW CONCURRENTLY hourly_stats;
END;
$$ LANGUAGE plpgsql;

-- Setup cron job (requires pg_cron extension)
SELECT cron.schedule('refresh-stats', '*/5 * * * *', 'SELECT refresh_hourly_stats()');
```


```typescript
// packages/snacks/src/tracking-exposed/ClickDNAAnalyzer.tsx
import { ClickDNAAnalyzer as V1 } from '@/current/Snacks/ClickDNAAnalyzer'
import { DataSnackSDK } from '@data-snack/tracking'
import { ConsentManager } from '@data-snack/core'

export function ClickDNAAnalyzerV2() {
  const sdk = new DataSnackSDK()
  const consent = new ConsentManager()
  
  // Wrap V1 component with new tracking
  return (
    <TrackingProvider sdk={sdk}>
      <ConsentGate required={['analytics', 'personalization']}>
        <V1 
          onEvent={(event, data) => {
            // Send to new tracking system
            sdk.track(`click_dna.${event}`, data)
          }}
        />
      </ConsentGate>
    </TrackingProvider>
  )
}
```

###  New Snack Development 


```typescript
// packages/snacks/src/tracking-exposed/PrivacyLeakDetector.tsx
import { SnackContainer } from '@data-snack/ui'
import { useTracking } from '@data-snack/tracking'
import { calculateUniquenesS } from '@data-snack/core'

export function PrivacyLeakDetector() {
  const { track } = useTracking()
  const [leaks, setLeaks] = useState<PrivacyLeak[]>([])
  const [analyzing, setAnalyzing] = useState(false)
  
  const detectLeaks = async () => {
    setAnalyzing(true)
    track('privacy_leak.start')
    
    const detected: PrivacyLeak[] = []
    
    // 1. Canvas Fingerprinting
    const canvas = document.createElement('canvas')
    const ctx = canvas.getContext('2d')!
    ctx.textBaseline = 'top' 
    ctx.font = '14px Arial'
    ctx.fillText('ğŸ”', 2, 2)
    const canvasData = canvas.toDataURL()
    detected.push({
      type: 'canvas_fingerprint',
      value: canvasData.substring(0, 50),
      uniqueness: 0.89,
      description: 'Your browser draws images uniquely'
    })
    
    // 2. WebGL Fingerprinting
    const gl = canvas.getContext('webgl')
    const debugInfo = gl?.getExtension('WEBGL_debug_renderer_info')
    if (debugInfo) {
      detected.push({
        type: 'webgl_gpu',
        value: gl.getParameter(debugInfo.UNMASKED_RENDERER_WEBGL),
        uniqueness: 0.95,
        description: 'Your graphics card is identifiable'
      })
    }
    
    // 3. Audio Context Fingerprinting
    const AudioContext = window.AudioContext || window.webkitAudioContext
    const audioCtx = new AudioContext()
    const oscillator = audioCtx.createOscillator()
    const analyser = audioCtx.createAnalyser()
    const gain = audioCtx.createGain()
    oscillator.connect(analyser)
    analyser.connect(gain)
    gain.connect(audioCtx.destination)
    oscillator.start(0)
    const audioData = new Float32Array(analyser.frequencyBinCount)
    analyser.getFloatFrequencyData(audioData)
    detected.push({
      type: 'audio_fingerprint',
      value: audioData.slice(0, 10).join(','),
      uniqueness: 0.76,
      description: 'Your audio stack has a signature'
    })
    
    // 4. Font Detection
    const fonts = await detectInstalledFonts()
    detected.push({
      type: 'installed_fonts',
      value: `${fonts.length} fonts detected`,
      uniqueness: 0.82,
      description: 'Your font list is unique'
    })
    
    // 5. Battery API
    if ('getBattery' in navigator) {
      const battery = await navigator.getBattery()
      detected.push({
        type: 'battery_status',
        value: `${battery.level * 100}% ${battery.charging ? 'charging' : 'discharging'}`,
        uniqueness: 0.43,
        description: 'Battery level narrows identity'
      })
    }
    
    setLeaks(detected)
    setAnalyzing(false)
    
    // Calculate overall uniqueness
    const overallUniqueness = calculateUniqueness(detected)
    track('privacy_leak.complete', {
      leaksFound: detected.length,
      uniqueness: overallUniqueness,
      marketValue: overallUniqueness * 15000
    })
  }
  
  return (
    <SnackContainer
      title="Privacy Leak Detector"
      subtitle="See what your browser reveals"
    >
      {/* Implementation */}
    </SnackContainer>
  )
}
```

## ğŸš¢ Deployment Strategy

### Development Environment
```yaml
# docker-compose.yml
version: '3.8'
services:
  web:
    build: ./apps/web
    ports:
      - 3000:3000
    environment:
      - DATABASE_URL=postgresql://...
      - GTM_SERVER_URL=http://tracking:8080
  
  tracking:
    build: ./services/tracking-server
    ports:
      - 8080:8080
    environment:
      - GTM_CONTAINER_ID=...
      - KAFKA_BROKER=kafka:9092
  
  kafka:
    image: confluentinc/cp-kafka:latest
    ports:
      - 9092:9092
    environment:
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
  
  postgres:
    image: supabase/postgres:14
    ports:
      - 5432:5432
    volumes:
      - ./infrastructure/sql:/docker-entrypoint-initdb.d
```

### Production Deployment
```typescript
// vercel.json
{
  "functions": {
    "app/api/track.ts": {
      "maxDuration": 10,
      "memory": 1024
    },
    "app/api/analyze.ts": {
      "maxDuration": 30,
      "memory": 3008
    }
  },
  "rewrites": [
    {
      "source": "/gtm/(.*)",
      "destination": "https://gtm-server.data-snack.com/$1"
    }
  ],
  "headers": [
    {
      "source": "/(.*)",
      "headers": [
        {
          "key": "X-Content-Type-Options",
          "value": "nosniff"
        },
        {
          "key": "Permissions-Policy",
          "value": "camera=(self), microphone=(self), geolocation=(self)"
        }
      ]
    }
  ]
}
```

## ğŸ“Š Success Metrics

### Technical KPIs
- **Build Time**: < 60s
- **Bundle Size**: < 500KB initial
- **Lighthouse Score**: > 95
- **Test Coverage**: > 80%
- **API Response**: < 200ms p95

### Business KPIs
- **Tracking Accuracy**: > 95% (vs 60-70% client-side)
- **Consent Rate**: > 70%
- **Snack Completion**: > 60%
- **Viral Coefficient**: > 1.2
- **User Retention**: > 25% DAU/MAU

## ğŸ—“ï¸ Timeline

### Foundation
- âœ… Monorepo setup
- âœ… Core packages
- âœ… Database schema
- âœ… CI/CD pipeline

### Tracking and Analyticd
- âœ… Server-side tracking
- âœ… Consent management
- âœ… Testing setup

### New Features
- âœ… Privacy Leak Detector
- âœ… Scroll Behavior Decoder
- âœ… Social Media Addiction Radar
- âœ… Documentation site

### Launch Prep
- âœ… Performance optimization
- âœ… Security audit
- âœ… Load testing
- âœ… Marketing materials

## ğŸ¯ Next Immediate Steps

1. **Create new repository structure**
```bash
mkdir data-snack-v2 && cd data-snack-v2
pnpm init
pnpm add -D turbo @changesets/cli
```

2. **Setup core packages**
```bash
pnpm create @data-snack/core
pnpm create @data-snack/ui
pnpm create @data-snack/tracking
```


3. **Setup GTM Server Container**
```bash
cd services/tracking-server
npm init -y
npm install @google/tag-manager-server express
```

5. **Configure Supabase**
```bash
supabase init
supabase db push
```

This architecture provides:
- âœ… **Scalability** through microservices
- âœ… **Maintainability** through monorepo
- âœ… **Privacy** through server-side tracking
- âœ… **Performance** through optimized bundles
- âœ… **Flexibility** through modular packages

Ready to start implementation!